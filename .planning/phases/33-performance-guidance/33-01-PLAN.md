---
phase: 33-performance-guidance
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - Private/Write-LabPerformanceMetric.ps1
  - Private/Get-LabPerformanceConfig.ps1
  - Public/Measure-LabVMOperation.ps1
  - Tests/LabPerformanceMetrics.Tests.ps1
autonomous: true
requirements:
  - PERF-01
  - PERF-03

must_haves:
  truths:
    - "Performance data is collected automatically for VM operations"
    - "Metrics include provision time, snapshot duration, and operation timestamps"
    - "Performance data is stored in JSON format for historical analysis"
    - "Operator can manually trigger performance measurement for any operation"
    - "Collection is non-blocking and logs errors without throwing"
  artifacts:
    - path: "Private/Write-LabPerformanceMetric.ps1"
      provides: "Performance metric writing to storage"
      min_lines: 80
    - path: "Private/Get-LabPerformanceConfig.ps1"
      provides: "Performance storage configuration"
      min_lines: 40
    - path: "Public/Measure-LabVMOperation.ps1"
      provides: "Public API for manual performance measurement"
      exports: ["Measure-LabVMOperation"]
      min_lines: 70
    - path: "Tests/LabPerformanceMetrics.Tests.ps1"
      provides: "Unit tests for performance metric collection"
      min_lines: 60
  key_links:
    - from: "Measure-LabVMOperation.ps1"
      to: "Write-LabPerformanceMetric.ps1"
      via: "Calls metric writer after operation completes"
      pattern: "Write-LabPerformanceMetric"
    - from: "Write-LabPerformanceMetric.ps1"
      to: "Get-LabPerformanceConfig.ps1"
      via: "Gets storage path from configuration"
      pattern: "Get-LabPerformanceConfig"
    - from: "Private/Write-LabRunArtifacts.ps1"
      to: "Write-LabPerformanceMetric.ps1"
      via: "Existing artifact writing pattern for reference"
      pattern: "Write-LabRunArtifacts"
---

<objective>
Create automatic performance metric collection infrastructure that tracks VM operation durations (provision time, snapshot duration, startup/shutdown times) and stores metrics in JSON format for historical analysis and trend tracking.

Purpose: Enable performance visibility by automatically collecting timing data for VM operations, allowing operators to understand how long operations take and identify performance trends or degradations over time.

Output: Performance metric storage configuration, metric writer function that stores operation timing data, public API for manual performance measurement, and unit tests covering metric collection and storage.
</objective>

<execution_context>
@/home/anthonyscry/.claude/get-shit-done/workflows/execute-plan.md
@/home/anthonyscry/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Existing patterns from codebase (for reference, not to copy)
Private/Write-LabRunArtifacts.ps1 - Shows run artifact JSON storage pattern with duration tracking
Private/Write-LabAnalyticsEvent.ps1 - Shows event writing with non-blocking error handling
Private/Invoke-LabBulkOperationCore.ps1 - Shows stopwatch pattern for operation timing
Public/Invoke-LabBulkOperation.ps1 - Shows how bulk operations track duration
Private/Get-LabAnalyticsConfig.ps1 - Shows config retrieval pattern
</context>

<tasks>

<task type="auto">
  <name>Create Get-LabPerformanceConfig configuration function</name>
  <files>Private/Get-LabPerformanceConfig.ps1</files>
  <action>
Create Private/Get-LabPerformanceConfig.ps1 with performance configuration:

```powershell
function Get-LabPerformanceConfig {
    <#
    .SYNOPSIS
        Retrieves performance metrics configuration.

    .DESCRIPTION
        Get-LabPerformanceConfig returns performance storage configuration including
        enabled status, storage path, and retention settings. Configuration is
        stored in .planning/performance-config.json and created with defaults if
        missing.

    .OUTPUTS
        [pscustomobject] with Enabled (bool), StoragePath (string), RetentionDays (int).
    #>
    [CmdletBinding()]
    [OutputType([pscustomobject])]
    param()

    $configPath = Join-Path $PSScriptRoot '.planning\performance-config.json'
    $defaultStoragePath = Join-Path $PSScriptRoot '.planning\performance-metrics.json'

    if (Test-Path $configPath) {
        try {
            $config = Get-Content -Raw -Path $configPath | ConvertFrom-Json
            return [pscustomobject]@{
                Enabled       = [bool]$config.Enabled
                StoragePath   = $config.StoragePath ?? $defaultStoragePath
                RetentionDays = $config.RetentionDays ?? 90
            }
        }
        catch {
            Write-Warning "Get-LabPerformanceConfig: Failed to read config from '$configPath' - $_"
        }
    }

    # Create default config
    $defaultConfig = [pscustomobject]@{
        Enabled       = $true
        StoragePath   = $defaultStoragePath
        RetentionDays = 90
    }

    try {
        $configDir = Split-Path -Parent $configPath
        if (-not (Test-Path $configDir)) {
            $null = New-Item -Path $configDir -ItemType Directory -Force
        }
        $defaultConfig | ConvertTo-Json | Set-Content -Path $configPath -Encoding UTF8
    }
    catch {
        Write-Warning "Get-LabPerformanceConfig: Failed to write default config to '$configPath' - $_"
    }

    return $defaultConfig
}
```

This function provides performance storage configuration, creating a default config file if none exists. It follows the same pattern as Get-LabAnalyticsConfig from Phase 30, ensuring consistency across the codebase. The default storage location is .planning/performance-metrics.json, and retention defaults to 90 days.
  </action>
  <verify>Get-LabPerformanceConfig returns object with Enabled, StoragePath, and RetentionDays properties</verify>
  <done>Get-LabPerformanceConfig function exists and returns configuration</done>
</task>

<task type="auto">
  <name>Create Write-LabPerformanceMetric function</name>
  <files>Private/Write-LabPerformanceMetric.ps1</files>
  <action>
Create Private/Write-LabPerformanceMetric.ps1 with metric writing logic:

```powershell
function Write-LabPerformanceMetric {
    <#
    .SYNOPSIS
        Writes a performance metric record to the performance log.

    .DESCRIPTION
        Write-LabPerformanceMetric appends performance data for VM operations to
        the performance metrics JSON file. Records include operation type, VM name,
        duration, start time, end time, and optional metadata. Non-blocking operation
        that logs errors without throwing.

    .PARAMETER Operation
        Operation type: 'Provision', 'Start', 'Stop', 'Suspend', 'Checkpoint',
        'Restore', 'Remove', 'NetworkInit', 'DomainJoin'.

    .PARAMETER VMName
        Name of the VM the operation was performed on.

    .PARAMETER Duration
        Duration of the operation as TimeSpan.

    .PARAMETER StartTime
        When the operation started (DateTime).

    .PARAMETER EndTime
        When the operation completed (DateTime).

    .PARAMETER Metadata
        Additional metadata as hashtable (optional).

    .PARAMETER LabName
        Name of the lab (optional).

    .EXAMPLE
        Write-LabPerformanceMetric -Operation 'Start' -VMName 'dc1' -Duration $ts -StartTime $st -EndTime $et
    #>
    [CmdletBinding()]
    param(
        [Parameter(Mandatory)]
        [ValidateSet('Provision', 'Start', 'Stop', 'Suspend', 'Checkpoint', 'Restore', 'Remove', 'NetworkInit', 'DomainJoin', 'BulkOperation', 'Workflow')]
        [string]$Operation,

        [Parameter(Mandatory)]
        [string]$VMName,

        [Parameter(Mandatory)]
        [TimeSpan]$Duration,

        [Parameter(Mandatory)]
        [DateTime]$StartTime,

        [Parameter(Mandatory)]
        [DateTime]$EndTime,

        [hashtable]$Metadata = @(),

        [string]$LabName
    )

    try {
        $perfConfig = Get-LabPerformanceConfig

        if (-not $perfConfig.Enabled) {
            return
        }

        $storagePath = $perfConfig.StoragePath
        $parentDir = Split-Path -Parent $storagePath

        if (-not [string]::IsNullOrWhiteSpace($parentDir) -and -not (Test-Path $parentDir)) {
            $null = New-Item -Path $parentDir -ItemType Directory -Force
            Write-Verbose "Created directory: $parentDir"
        }

        $metric = [pscustomobject]@{
            Timestamp      = Get-Date -Format 'o'
            Operation      = $Operation
            VMName         = $VMName
            LabName        = $LabName
            DurationMs     = [int]$Duration.TotalMilliseconds
            DurationSec    = [math]::Round($duration.TotalSeconds, 3)
            StartTime      = $StartTime.ToUniversalTime().ToString('o')
            EndTime        = $EndTime.ToUniversalTime().ToString('o')
            Host           = $env:COMPUTERNAME
            User           = "$env:USERDOMAIN\$env:USERNAME"
            Metadata       = if ($Metadata.Count -gt 0) { $Metadata } else { $null }
        }

        # Load existing metrics or create new
        if ((Test-Path $storagePath)) {
            $existing = Get-Content -Raw -Path $storagePath | ConvertFrom-Json
            if ($existing.metrics) {
                $existing.metrics += @($metric)
            } else {
                $existing = [pscustomobject]@{ metrics = @($metric) }
            }
        } else {
            $existing = [pscustomobject]@{ metrics = @($metric) }
        }

        # Apply retention - keep only metrics within retention period
        $cutoffDate = (Get-Date).AddDays(-($perfConfig.RetentionDays))
        $existing.metrics = @($existing.metrics | Where-Object {
            try {
                $metricTime = [DateTime]::Parse($_.Timestamp)
                $metricTime -ge $cutoffDate
            }
            catch {
                $true
            }
        })

        $existing | ConvertTo-Json -Depth 8 | Set-Content -Path $storagePath -Encoding UTF8
        Write-Verbose "Write-LabPerformanceMetric: Logged $operation on $VMName ($($DurationSec)s)"
    }
    catch {
        Write-Warning "Write-LabPerformanceMetric: failed to write metric to '$storagePath' - $_"
    }
}
```

This function writes performance metrics to JSON storage with automatic retention cleanup. It's non-blocking (catches and warns on errors) and supports multiple operation types. The metric structure includes both milliseconds and seconds for duration precision and readability. Retention is applied automatically when writing new metrics to keep storage bounded.
  </action>
  <verify>Write-LabPerformanceMetric -Operation 'Start' -VMName 'testvm' -Duration [TimeSpan]::FromSeconds(5) -StartTime (Get-Date).AddSeconds(-5) -EndTime (Get-Date) creates entry in .planning/performance-metrics.json</verify>
  <done>Write-LabPerformanceMetric function exists and writes metrics to storage</done>
</task>

<task type="auto">
  <name>Create Measure-LabVMOperation public API</name>
  <files>Public/Measure-LabVMOperation.ps1</files>
  <action>
Create Public/Measure-LabVMOperation.ps1 as the public API:

```powershell
function Measure-LabVMOperation {
    <#
    .SYNOPSIS
        Measures and records performance of a VM operation.

    .DESCRIPTION
        Measure-LabVMOperation executes a scriptblock and records performance
        metrics including duration, start/end times, and success status. Metrics
        are automatically stored for historical analysis. Useful for manual
        performance testing or benchmarking VM operations.

    .PARAMETER VMName
        Name of the VM being operated on.

    .PARAMETER Operation
        Type of operation being measured.

    .PARAMETER ScriptBlock
        Scriptblock to execute and measure.

    .PARAMETER LabName
        Name of the lab (optional).

    .PARAMETER Metadata
        Additional metadata to record with the metric (optional).

    .EXAMPLE
        Measure-LabVMOperation -VMName 'dc1' -Operation 'Start' -ScriptBlock { Start-VM -Name 'dc1' }

    .EXAMPLE
        $metrics = Measure-LabVMOperation -VMName 'svr1' -Operation 'Checkpoint' -ScriptBlock {
            Checkpoint-VM -Name 'svr1' -SnapshotName 'test'
        }
    #>
    [CmdletBinding(SupportsShouldProcess)]
    [OutputType([pscustomobject])]
    param(
        [Parameter(Mandatory)]
        [string]$VMName,

        [Parameter(Mandatory)]
        [ValidateSet('Provision', 'Start', 'Stop', 'Suspend', 'Checkpoint', 'Restore', 'Remove', 'NetworkInit', 'DomainJoin')]
        [string]$Operation,

        [Parameter(Mandatory)]
        [scriptblock]$ScriptBlock,

        [string]$LabName,

        [hashtable]$Metadata = @{}
    )

    $startTime = Get-Date
    $errorOccurred = $false
    $errorMessage = $null

    try {
        Write-Verbose "Measure-LabVMOperation: Starting $operation on $VMName"
        & $ScriptBlock
    }
    catch {
        $errorOccurred = $true
        $errorMessage = $_.Exception.Message
        throw
    }
    finally {
        $endTime = Get-Date
        $duration = $endTime - $startTime

        $metricMetadata = @{
            Success = -not $errorOccurred
        }

        if ($errorOccurred) {
            $metricMetadata.Error = $errorMessage
        }

        foreach ($key in $Metadata.Keys) {
            $metricMetadata[$key] = $Metadata[$key]
        }

        Write-LabPerformanceMetric -Operation $Operation -VMName $VMName -Duration $duration -StartTime $startTime -EndTime $endTime -Metadata $metricMetadata -LabName $LabName
    }

    return [pscustomobject]@{
        VMName    = $VMName
        Operation = $Operation
        Duration  = $duration
        StartTime = $startTime
        EndTime   = $endTime
        Success   = -not $errorOccurred
        Error     = $errorMessage
    }
}
```

This public API provides a convenient way to measure any VM operation while automatically recording the performance data. It wraps scriptblocks in timing logic and writes metrics even when operations fail (recording the error in metadata). The returned object provides immediate feedback while the metric is stored for historical analysis.
  </action>
  <verify>Measure-LabVMOperation -VMName 'test' -Operation 'Start' -ScriptBlock { Start-Sleep -Milliseconds 100 } returns object with Duration property and creates metric record</verify>
  <done>Measure-LabVMOperation public API exists for manual performance measurement</done>
</task>

<task type="auto">
  <name>Create unit tests for performance metrics</name>
  <files>Tests/LabPerformanceMetrics.Tests.ps1</files>
  <action>
Create Tests/LabPerformanceMetrics.Tests.ps1 with unit tests:

```powershell
Describe 'Write-LabPerformanceMetric' {
    BeforeAll {
        $moduleRoot = Split-Path -Parent (Split-Path -Parent $PSScriptRoot)
        Import-Module "$moduleRoot\SimpleLab\SimpleLab.psd1" -Force

        . "$moduleRoot\Private\Get-LabPerformanceConfig.ps1"
        . "$moduleRoot\Private\Write-LabPerformanceMetric.ps1"

        $testStoragePath = Join-Path $TestDrive 'test-perf-metrics.json'
    }

    Context 'Configuration retrieval' {
        It 'Returns default config when no config file exists' {
            $config = Get-LabPerformanceConfig

            $config.Enabled | Should -BeTrue
            $config.StoragePath | Should -Not -BeNullOrEmpty
            $config.RetentionDays | Should -Be 90
        }

        It 'Returns config with StoragePath property' {
            $config = Get-LabPerformanceConfig

            $config.StoragePath | Should -Not -BeNullOrEmpty
        }
    }

    Context 'Metric writing' {
        It 'Writes metric to storage file' {
            Mock Get-LabPerformanceConfig {
                return [pscustomobject]@{
                    Enabled       = $true
                    StoragePath   = $testStoragePath
                    RetentionDays = 90
                }
            }

            $startTime = Get-Date
            $duration = [TimeSpan]::FromSeconds(5.5)

            Write-LabPerformanceMetric -Operation 'Start' -VMName 'testvm' -Duration $duration -StartTime $startTime -EndTime $startTime.AddSeconds(5.5)

            Test-Path $testStoragePath | Should -BeTrue
        }

        It 'Includes all required metric properties' {
            Mock Get-LabPerformanceConfig {
                return [pscustomobject]@{
                    Enabled       = $true
                    StoragePath   = $testStoragePath
                    RetentionDays = 90
                }
            }

            $startTime = Get-Date

            Write-LabPerformanceMetric -Operation 'Provision' -VMName 'dc1' -Duration ([TimeSpan]::FromMinutes(2)) -StartTime $startTime -EndTime $startTime.AddMinutes(2) -LabName 'TestLab'

            $content = Get-Content -Raw -Path $testStoragePath | ConvertFrom-Json
            $content.metrics.Count | Should -BeGreaterOrEqual 1

            $metric = $content.metrics[0]
            $metric.Operation | Should -Be 'Provision'
            $metric.VMName | Should -Be 'dc1'
            $metric.LabName | Should -Be 'TestLab'
            $metric.DurationSec | Should -Be 120
        }

        It 'Appends multiple metrics to storage' {
            Mock Get-LabPerformanceConfig {
                return [pscustomobject]@{
                    Enabled       = $true
                    StoragePath   = $testStoragePath
                    RetentionDays = 90
                }
            }

            $startTime = Get-Date

            Write-LabPerformanceMetric -Operation 'Start' -VMName 'vm1' -Duration ([TimeSpan]::FromSeconds(1)) -StartTime $startTime -EndTime $startTime.AddSeconds(1)
            Write-LabPerformanceMetric -Operation 'Stop' -VMName 'vm1' -Duration ([TimeSpan]::FromSeconds(0.5)) -StartTime $startTime.AddSeconds(5) -EndTime $startTime.AddSeconds(5.5)

            $content = Get-Content -Raw -Path $testStoragePath | ConvertFrom-Json
            $content.metrics.Count | Should -BeGreaterOrEqual 2
        }

        It 'Does not write when performance collection disabled' {
            Mock Get-LabPerformanceConfig {
                return [pscustomobject]@{
                    Enabled       = $false
                    StoragePath   = $testStoragePath
                    RetentionDays = 90
                }
            }

            $startTime = Get-Date

            Write-LabPerformanceMetric -Operation 'Start' -VMName 'testvm' -Duration ([TimeSpan]::FromSeconds(1)) -StartTime $startTime -EndTime $startTime.AddSeconds(1)

            Test-Path $testStoragePath | Should -BeFalse
        }
    }
}

Describe 'Measure-LabVMOperation' {
    BeforeAll {
        $moduleRoot = Split-Path -Parent (Split-Path -Parent $PSScriptRoot)
        Import-Module "$moduleRoot\SimpleLab\SimpleLab.psd1" -Force
    }

    Context 'Operation measurement' {
        It 'Measures scriptblock execution time' {
            Mock Write-LabPerformanceMetric { }

            $result = Measure-LabVMOperation -VMName 'testvm' -Operation 'Start' -ScriptBlock {
                Start-Sleep -Milliseconds 100
            }

            $result.Duration.TotalMilliseconds | Should -BeGreaterOrEqual 100
            $result.Success | Should -BeTrue
        }

        It 'Records performance metric after execution' {
            Mock Write-LabPerformanceMetric { }

            Measure-LabVMOperation -VMName 'dc1' -Operation 'Stop' -ScriptBlock { }

            Should -Invoke Write-LabPerformanceMetric -Times 1 -Scope It
        }

        It 'Includes error in metric when scriptblock fails' {
            Mock Write-LabPerformanceMetric { }

            try {
                Measure-LabVMOperation -VMName 'testvm' -Operation 'Start' -ScriptBlock {
                    throw 'Test error'
                }
            }
            catch {
                # Expected
            }

            Should -Invoke Write-LabPerformanceMetric -Times 1 -Scope It -ParameterFilter {
                $Metadata.Success -eq $false -and $Metadata.Error -eq 'Test error'
            }
        }

        It 'Passes LabName and additional Metadata' {
            Mock Write-LabPerformanceMetric { }

            Measure-LabVMOperation -VMName 'svr1' -Operation 'Checkpoint' -LabName 'MyLab' -Metadata @{ SnapshotName = 'baseline' } -ScriptBlock { }

            Should -Invoke Write-LabPerformanceMetric -Times 1 -Scope It -ParameterFilter {
                $LabName -eq 'MyLab' -and $Metadata.Count -ge 1
            }
        }
    }
}
```

These tests verify the performance metric collection infrastructure. Tests cover configuration retrieval, metric writing, multiple metrics, disabled state, and the public Measure-LabVMOperation API including error handling and metadata passing. The test pattern follows Pester 5 conventions with proper mocking.
  </action>
  <verify>Invoke-Pester Tests/LabPerformanceMetrics.Tests.ps1 -PassThru | Select-Object -ExpandProperty PassedCount is greater than 0</verify>
  <done>Unit tests exist and verify performance metric collection</done>
</task>

<task type="auto">
  <name>Add functions to module exports and auto-discovery</name>
  <files>SimpleLab.psm1, SimpleLab.psd1</files>
  <action>
Update SimpleLab.psm1 and SimpleLab.psd1 to export Measure-LabVMOperation.

In SimpleLab.psm1, add 'Measure-LabVMOperation' to the Export-ModuleMember array.
In SimpleLab.psd1, add 'Measure-LabVMOperation' to the FunctionsToExport array.

Private functions (Write-LabPerformanceMetric, Get-LabPerformanceConfig) are auto-discovered via Lab-Common.ps1's pattern that sources all Private/*.ps1 files. No changes to Lab-Common.ps1 are needed.

This follows the established pattern for public API functions from previous phases.
  </action>
  <verify>Get-Command Measure-LabVMOperation -Module SimpleLab returns command info</verify>
  <done>Measure-LabVMOperation is exported from the SimpleLab module</done>
</task>

</tasks>

<verification>
1. Import module: `Import-Module ./SimpleLab/SimpleLab.psd1` succeeds
2. Test config: Get-LabPerformanceConfig returns configuration object
3. Test metric writing: Write-LabPerformanceMetric creates JSON entries
4. Test public API: Measure-LabVMOperation measures scriptblock execution
5. Test retention: Old metrics are automatically cleaned up
6. Test disabled state: Metrics are not written when collection disabled
7. Run tests: Pester tests for performance metrics pass
</verification>

<success_criteria>
1. Get-LabPerformanceConfig returns config with Enabled, StoragePath, RetentionDays
2. Write-LabPerformanceMetric writes metrics to JSON with all required fields
3. Metrics include Operation, VMName, DurationMs, DurationSec, StartTime, EndTime
4. Retention is automatically applied when writing new metrics
5. Measure-LabVMOperation measures and records operation timing
6. Failed operations are recorded with error information
7. Unit tests verify metric collection, configuration, and public API
8. Measure-LabVMOperation is exported from SimpleLab module
</success_criteria>

<output>
After completion, create `.planning/phases/33-performance-guidance/33-01-SUMMARY.md` with:
- Actual files created with line counts
- Performance metric schema documentation
- Test coverage summary
- Any deviations from plan
- Next steps for Plan 33-02
</output>
